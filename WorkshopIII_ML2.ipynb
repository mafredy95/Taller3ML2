{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab III - Advanced Topics - Machine Learning II\n",
    "\n",
    "Estudiantes: \n",
    "- Edward Alejandro Giraldo Gall√≥n\n",
    "- Mafredy Acevedo Holgu√≠n\n",
    "- Laura Cristina Torres L√≥pez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\n",
    "In your own words, describe what vector embeddings are and what they are useful for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Son representaciones de palabras o sentencias mediante arreglos num√©ricos de menor dimensionalidad, que guardan su sentido y su sem√°ntica. Estos vectores son empleados por algoritmos para el procesamiento del lenguaje natural, mediante la comparaci√≥n de similitud entre vectores, por medio de criterios de distancia de acuerdo a la aplicaci√≥n. Esta aproximaci√≥n resulta m√°s adecuada, debido a que la comparaci√≥n directa entre palabras o sentencias resulta ser de alta complejidad y costo computacional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\n",
    "What do you think is the best distance criterion to estimate how far two embeddings (vectors) are from each other? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De acuerdo con la literatura investigada, se encuentran dos tipos grandes categorias de evaluadores para estimar que tan lejos est√° un vector embebido de otro:  \n",
    "\n",
    "* Evaluadores intr√≠nsecos:\n",
    "\n",
    "    Buscan medir la calidad propia del embebimiento de las palabras o sentencias a trav√©s de tareas propias del procesamiento de estas, como similiridad entre palabras, analog√≠as y clasificaci√≥n. Estas mediciones se hacen directamente sobre los vectores sin haberlos usado en tareas del mundo real. Algunas de las m√©tricas m√°s usadas son la similaridad coseno, la correlaci√≥n de Spearman y la precisi√≥n, siendo la similaridad coseno la m√°s usada debido a m√∫ltiples razones, tales como su independencia a la relevancia de las palabras o sentencias, su robustez respecto a la escala y su bajo costo computacional. Este m√©todo mide la similitud entre vectores a trav√©s del coseno del √°ngulo entre ellos. El problema de este tipo de m√©todos recae en que pueden confundir similitud con relacionamiento, por lo que no llegan a ser apropiados para todo tipo de evaluaciones.\n",
    "\n",
    "* Evaluadores extr√≠nsecos:\n",
    "\n",
    "    Buscan medir el desempe√±o de los vectores embebidos usados como caracter√≠sicas de entrada para tareas del mundo real (procesamiento de texto, traducciones, etc...). Con esto se logra conocer como las representaciones num√©ricas obtenidas afecta el rendimiento de tareas espec√≠ficas paras las cuales son usados y entender la utilidad de los vectores de acuerdo al contexto. Algunos de las m√©tricas empleadas son F1 Score y Perplexity.\n",
    "\n",
    "\n",
    " En resumen, no existe una m√©trica universal que sea mejor que las dem√°s. Todo depender√° de que se pretende evaluar: la calidad del embebemiento en s√≠ o su utilidad en alguna aplicaci√≥n espec√≠fica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)\n",
    "Let us build a Q&A (question answering) system! üòÄFor this, consider the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias a usar:\n",
    "\n",
    "#Estructuras de datos:\n",
    "import numpy as np\n",
    "\n",
    "#Para trabajar con NLP (Natural Lenguage Processing)\n",
    "\n",
    "##Natural Language Toolkit\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize#, word_tokenize\n",
    "\n",
    "#Hugging Face\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "#Criterios de distancia entre vectores embebidos\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Pick whatever text you like, in the order of 20+ paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se escoge un texto acerca de la segunda guerra mundial\n",
    "with open('WW2.txt', 'r', encoding='utf-8') as archivo:\n",
    "    # Lee todo el contenido del archivo\n",
    "    texto = archivo.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Split that text into meaningful chunks/pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['World War II, also known as the Second World War, was a global conflict that lasted from 1939 to 1945.', 'It was triggered by the invasion of Poland by Nazi Germany in 1939, which led to the United Kingdom and France declaring war on Germany.', 'This marked the beginning of the war in Europe.', 'The war expanded as Germany invaded the Soviet Union in 1941, creating a second front in the East.', \"Meanwhile, Japan's attack on Pearl Harbor in December 1941 brought the United States into the conflict, leading to a Pacific theater of war.\", 'The Axis powers, including Germany, Italy, and Japan, fought against the Allied powers, which included the United States, Great Britain, the Soviet Union, and others.', 'The war involved over 30 countries and resulted in widespread devastation.', 'The Holocaust, in which millions of Jews and others were systematically murdered by the Nazis, remains one of the most horrifying aspects of World War II.', 'Concentration camps and extermination camps were used to carry out these atrocities.', 'The Battle of Stalingrad in 1942-1943 marked a turning point on the Eastern Front.', 'The Soviet Union successfully defended the city, leading to a series of Soviet offensives that pushed the Germans back.', 'In the Western Front, the D-Day invasion in 1944 allowed the Allies to establish a foothold in Normandy, leading to the liberation of Western Europe from Nazi occupation.', 'The Pacific theater saw significant battles like the Battle of Midway and the Battle of Iwo Jima.', \"The use of atomic bombs on Hiroshima and Nagasaki in 1945 led to Japan's surrender.\", 'World War II officially ended in September 1945 with the signing of the Japanese Instrument of Surrender aboard the USS Missouri.', 'The war had a profound impact on the world.', 'The aftermath of World War II saw the establishment of the United Nations in 1945, aimed at promoting peace and international cooperation.', 'It also led to the division of Germany and the onset of the Cold War between the United States and the Soviet Union.', 'The war resulted in significant territorial changes and decolonization as European empires weakened.', 'The Nuremberg Trials held in 1945-1946 prosecuted Nazi war criminals.', 'The human cost of World War II was immense, with millions of lives lost and many more wounded.', 'It left scars on generations and reshaped the global political landscape.', \"The Marshall Plan, initiated by the United States, provided aid to help rebuild Europe after the war, contributing to the continent's recovery.\", \"The war's impact extended beyond its conclusion, with lasting effects on technology, culture, and geopolitics.\", 'It led to the emergence of superpowers and the beginning of the Cold War.', 'The post-war era saw the establishment of the European Union and the North Atlantic Treaty Organization (NATO) as organizations aimed at promoting stability and cooperation.', 'The lessons learned from World War II have influenced international diplomacy and conflict resolution efforts in the decades since its conclusion.', 'World War II remains a subject of extensive historical research, literature, and remembrance, ensuring that its lessons and legacy continue to be studied and understood.', \"The war's impact on art, cinema, and literature is profound, with many works of fiction and non-fiction exploring its themes and consequences.\", 'The heroic actions of individuals and groups during World War II, such as the Resistance movements and Allied forces, are celebrated and remembered for their contributions to victory.', 'Memorials and museums dedicated to World War II are located around the world, serving as reminders of the sacrifices made and the importance of preventing such a devastating conflict in the future.', 'In conclusion, World War II was a pivotal event in history that shaped the course of the 20th century.', 'Its enduring impact on politics, society, and culture continues to be felt today, making it a subject of continued study and reflection.']\n"
     ]
    }
   ],
   "source": [
    "sentencias=sent_tokenize(texto)\n",
    "print(sentencias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Implement the embedding generation logic. Which tools and approaches would help you generate them easily and high-level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44368e6bd0024d869cc32e3d9631ddac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Usuario\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ac7fb1bc6c410ebef3cfb2efcab19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e049de826e487bb6033505b8568541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8afb2de479154be1bc203509f67d81e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/501M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Cargar el modelo RoBERTa preentrenado y el tokenizador\n",
    "model_name = 'roberta-base'  # Se puede elegir otro modelo si se desea. Para este caso se implement√≥ el modelo Base de RoBERTa, \n",
    "#que es un modelo pre-entrenado que usa MLM (Masked Language Modeling) \n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeber las sentencias\n",
    "embeddings = []\n",
    "\n",
    "for sentence in sentencias[:10]:\n",
    "    # Tokenizar y convertir cada sentencia a tensores\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Obtener los embeddings\n",
    "    with torch.no_grad():  # Para evitar que se realice el c√°lculo de gradientes\n",
    "        outputs = model(**inputs)\n",
    "        sentence_embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "    \n",
    "    # Agregar el embedding a la lista\n",
    "    embeddings.append(sentence_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar y convertir la pregunta 1 a tensor\n",
    "questions=['What was the World War II?','When did the war end?','What was The Holocaust?']\n",
    "question_token = tokenizer(questions, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Obtener los embeddings de la pregunta 1\n",
    "with torch.no_grad():  # Para evitar que se realice el c√°lculo de gradientes\n",
    "    outputs = model(**question_token)\n",
    "    question_emb = outputs.last_hidden_state.mean(dim=1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d)\n",
    "For every question asked by the user, return a sorted list of the N chunks/pieces in your text that\n",
    "relate the most to the question. Do results make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta 1): What was the World War II?\n",
      "La respuesta #1 es:\n",
      " \t World War II, also known as the Second World War, was a global conflict that lasted from 1939 to 1945.\n",
      "La respuesta #2 es:\n",
      " \t This marked the beginning of the war in Europe.\n",
      "La respuesta #3 es:\n",
      " \t The war expanded as Germany invaded the Soviet Union in 1941, creating a second front in the East.\n",
      "La respuesta #4 es:\n",
      " \t Meanwhile, Japan's attack on Pearl Harbor in December 1941 brought the United States into the conflict, leading to a Pacific theater of war.\n",
      "La respuesta #5 es:\n",
      " \t The Battle of Stalingrad in 1942-1943 marked a turning point on the Eastern Front.\n",
      "\n",
      "\n",
      "Pregunta 2): When did the war end?\n",
      "La respuesta #1 es:\n",
      " \t This marked the beginning of the war in Europe.\n",
      "La respuesta #2 es:\n",
      " \t The war involved over 30 countries and resulted in widespread devastation.\n",
      "La respuesta #3 es:\n",
      " \t World War II, also known as the Second World War, was a global conflict that lasted from 1939 to 1945.\n",
      "La respuesta #4 es:\n",
      " \t The Battle of Stalingrad in 1942-1943 marked a turning point on the Eastern Front.\n",
      "La respuesta #5 es:\n",
      " \t The Holocaust, in which millions of Jews and others were systematically murdered by the Nazis, remains one of the most horrifying aspects of World War II.\n",
      "\n",
      "\n",
      "Pregunta 3): What was The Holocaust?\n",
      "La respuesta #1 es:\n",
      " \t The Holocaust, in which millions of Jews and others were systematically murdered by the Nazis, remains one of the most horrifying aspects of World War II.\n",
      "La respuesta #2 es:\n",
      " \t The war involved over 30 countries and resulted in widespread devastation.\n",
      "La respuesta #3 es:\n",
      " \t World War II, also known as the Second World War, was a global conflict that lasted from 1939 to 1945.\n",
      "La respuesta #4 es:\n",
      " \t This marked the beginning of the war in Europe.\n",
      "La respuesta #5 es:\n",
      " \t The war expanded as Germany invaded the Soviet Union in 1941, creating a second front in the East.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cosine_scores=[]\n",
    "for i,Q in enumerate(questions):\n",
    "    cosine_scores.append([])\n",
    "    for sentence_emb in embeddings:\n",
    "        cosine_scores[i].append(cosine_similarity(sentence_emb,question_emb[i].reshape(1, -1))[0][0])\n",
    "    pos=np.argsort(np.array(cosine_scores[i]))[::-1]\n",
    "    print(f'Pregunta {i+1}): {Q}')\n",
    "    for i,p in enumerate(pos[:5]):\n",
    "        print(f'La respuesta #{i+1} es:\\n \\t {sentencias[p]}')\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las respuestas obtenidas tienen sentido, siempre y cuando la pregunta realizada est√© contenida en cierta proporci√≥n dentro de la respuesta, como lo son los casos de las preguntas 1) y 3). Para la pregunta 2), en la cual se utilizaron sin√≥nimos y palabras que no est√°n precisamente en la respuesta esperada, se obtuvieron respuestas sin alguna relaci√≥n con la consulta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4)\n",
    "What do you think that could make these types of systems more robust in terms of semantics and functionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, se listan algunos aspectos que pueden ayudar a robustecer la funcionalidad y la sem√°ntica de este tipo de sistemas:\n",
    "\n",
    "- Una de las formas de hacer el sistema m√°s robusto puede ser agregar diccionarios que contegan sin√≥nimos, ant√≥nimos, parafraseo de algunas oraciones comunes, con el fin de enriquecer la base de datos de tal forma que el algoritmo pueda entender mejor la sem√°ntica de las sentencias que embebe.\n",
    "\n",
    "- Agregar funcionalidades que permitan entender errores gram√°ticales.\n",
    "\n",
    "- Verificar previamente antes de la implementaci√≥n de los modelos, la calidad del embebimiento mediante evaluaciones extr√≠nsecas del mismo.\n",
    "\n",
    "- Otra manera, es agregar formas de que el modelo aprenda y se realimente de las evaluaciones extr√≠nsecas que se realicen sobre √©l.\n",
    "\n",
    "- Realimentar el modelo constantemente, a medida de que se cuenta con m√°s data.\n",
    "\n",
    "- Implementar una b√∫squeda de respuesta basada en reglas en el cual el sistema funcione en una l√≥gica de palabras claves y entidades; asign√°ndo una prioridad o jerarqu√≠a a las reglas, incorporando reglas enfocadas a contextos y reglas de respaldo que indiquen un error.\n",
    "\n",
    "- Combinar diferentes m√©tricas que se acoplen bien a la naturaleza de los datos, de manera que sus diferentes caracter√≠sticas se complementen entre s√≠.\n",
    "\n",
    "- Algoritmos capaces de generar por si mismos nuevas ideas (sentencias, par√°fraseo, etc), que ayuden a aumentar la data de entrenamiento con la que contar√° el modelo.\n",
    "\n",
    "- Existen modelos creados con el objetivo de identificar la similitud sem√°ntica entre preguntas y respuestas. Hacer uso de estos modelos y entrenarlos con el conjunto de datos espec√≠ficos puede mejorar la precisi√≥n. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
